{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "416ba533-b707-412d-99f3-79b02b1f0054",
   "metadata": {},
   "source": [
    "#### Importação das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bcbace0f-9881-4353-a574-509c3be26789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a8d1f6-7a6c-4653-b9d4-9c2d376e0e19",
   "metadata": {},
   "source": [
    "#### Exemplo de Corpus (Coleção de Documentos)\n",
    "##### Textos escritos por Shakespeare na wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c52e9acd-7357-4276-9911-c2ef9ebe1277",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd17b38-7a08-45e2-b43c-d70e0999ccb8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Processando o Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acd355b8-afd0-444c-a031-d362b446b627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "# Criando conjunto de palabras frequêntes\n",
    "stoplist = set('for a of the and to in'.split(' '))\n",
    "\n",
    "# Convertendo cada documento para letras minúsculas, aplicando split em cada documento e filtrando palavras irrelevantes\n",
    "texts =[[word for word in document.lower().split() if word not in stoplist]\n",
    "       for document in text_corpus]\n",
    "\n",
    "# Contando frequências de palavras\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "        \n",
    "# Mantendo apenas as palavras que aparecem mais de uma vez\n",
    "processed_corpus = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "pprint.pprint(processed_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a41af8-6a27-4c1c-b1ba-eee1140c7035",
   "metadata": {},
   "source": [
    "#### Associação de cada palavra do corpus com um unico id, formando assim um dicionário que define o vocabulário de todas as palavras que o processamento conhece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea65ce26-2258-4853-9276-108e90de1ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...>\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc0eb9f-b3e2-4a00-b916-539d265c5724",
   "metadata": {},
   "source": [
    "#### Dicionário das palavras com seus respectivos ids, gerados a partir da tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db1449b7-61c9-4e8a-968c-cc07bf734e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computer': 0,\n",
      " 'eps': 8,\n",
      " 'graph': 10,\n",
      " 'human': 1,\n",
      " 'interface': 2,\n",
      " 'minors': 11,\n",
      " 'response': 3,\n",
      " 'survey': 4,\n",
      " 'system': 5,\n",
      " 'time': 6,\n",
      " 'trees': 9,\n",
      " 'user': 7}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5bde95-bb6b-490e-b52c-23773c2246ec",
   "metadata": {},
   "source": [
    "#### Exemplo utilizando a técnica bag-of-word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72845b89-1906-42f0-bfe6-28c48594b0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Primeira entrada da tupla cooresponde ao id do token do dicionário \n",
    "# Segunda entrada da tupla cooresponde a quantidade de vezes que a palavra aparece na sentença\n",
    "new_doc = \"Human computer interaction\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4981309d-93fb-415b-8f8b-13fef541d066",
   "metadata": {},
   "source": [
    "#### Convertendo corpus de entrada original para lista de vetores utilizando técnica bag-of-word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2787026d-2bdd-4547-b6ba-17e90a0bcf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)],\n",
      " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
      " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
      " [(1, 1), (5, 2), (8, 1)],\n",
      " [(3, 1), (6, 1), (7, 1)],\n",
      " [(9, 1)],\n",
      " [(9, 1), (10, 1)],\n",
      " [(9, 1), (10, 1), (11, 1)],\n",
      " [(4, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "pprint.pprint(bow_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a34be1-919c-41b0-9e26-52fd18b4275d",
   "metadata": {},
   "source": [
    "#### Transformando o corpus depois de vetorizado utilizando o modelo tf-idf\n",
    "##### O modelo tf-idf transforma vetores da representação bag-of-words em um espaço vetorial onde as contagens de frequências são ponderadas de acordo com a raridade relativa de cada palavra no corpus.\n",
    "##### Palavras que aparecem mais vezes por exemplo \"system\" tem pesos menores do que palavras que aparecem menos vezes por exemplo \"minors\". Subtende-se que palavras que aparecem menos vezes tem mais representatividade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1930aac9-4ae0-41d1-b4b9-ed633ab9ad25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 0.5898341626740045), (11, 0.8075244024440723)]\n"
     ]
    }
   ],
   "source": [
    "# treinando o modelo\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "# transformando \"system minors\"\n",
    "words = \"system minors\".lower().split()\n",
    "print(tfidf[dictionary.doc2bow(words)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef82a73-1f75-4cbf-afa3-a917d5a0c2eb",
   "metadata": {},
   "source": [
    "#### Transformando todo Corpus via TfIdf e indexando e preparando para consultas de similaridade:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb513165-006c-4d3d-b8c9-89cc5aae788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = similarities.SparseMatrixSimilarity(tfidf[bow_corpus], num_features=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b382132-45f5-43d6-9992-6b61c43a39fa",
   "metadata": {},
   "source": [
    "#### Consultando a similaridade de um novo documento em relação a todos documentos do corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a22e19ef-e80a-4d3a-8d08-8c1db9b51fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.0), (1, 0.32448703), (2, 0.41707572), (3, 0.7184812), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "query_document = \"system engineering\".split()\n",
    "query_bow = dictionary.doc2bow(query_document)\n",
    "sims = index[tfidf[query_bow]]\n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d7e54d-03c4-4dac-9543-48bd99173808",
   "metadata": {},
   "source": [
    "#### Visualizando scores de similaridade relacionando com respectivo documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "377b96fe-27e6-43e5-be64-64877100a995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.7184812\n",
      "2 0.41707572\n",
      "1 0.32448703\n",
      "0 0.0\n",
      "4 0.0\n",
      "5 0.0\n",
      "6 0.0\n",
      "7 0.0\n",
      "8 0.0\n"
     ]
    }
   ],
   "source": [
    "for document_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):\n",
    "    print(document_number, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c502e851-6128-4696-8a8d-8f6116c28878",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
